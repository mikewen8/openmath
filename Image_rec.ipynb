{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f623ed9-93b9-4b0f-9ade-8efef1cb4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor flow guide book https://archive.org/details/ai-ml/page/24/mode/2up\n",
    "import tensorflow as tf # for ML and we will be using a tensorflow data set/ the mnist number data set\n",
    "import numpy as np #working with arrays\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import cv2 #for computer vision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a67d161c-3310-4795-ba01-371b378c69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = tf.keras.datasets.mnist #this is a training dataset generated by MNIST for number recoginition first data set made by mnist\n",
    "#due to this data being already split up in test adn training data we can call this below without splitting or data preprocessing\n",
    "(x_train, y_train), (x_test, y_test)= _data.load_data() #x is the pixel data, y is the classification/number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deb6f413-7a16-4055-87a2-3469a87187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets normalize the data/grey scaling \n",
    "\"\"\"gray scale pixel has value of  255  normalizing would make it between 0 and 1 \n",
    "it will be scaling things down the value of 125 might look like 0.5\"\"\"\n",
    "\n",
    "\"\"\"axis=-1:\n",
    "When axis=-1, normalization is performed along the last axis of the input data.\n",
    "In the context of a 2D tensor (like a matrix), where the first axis corresponds to rows and the second axis corresponds to columns, axis=-1 implies normalization is performed along the columns.\n",
    "This means that each feature (column) in the input data is independently normalized across the samples.\n",
    "axis=1:\n",
    "When axis=1, normalization is performed along the second axis of the input data.\n",
    "In the context of a 2D tensor, where the first axis corresponds to rows and the second axis corresponds to columns, axis=1 implies normalization is performed along the rows.\n",
    "This means that each sample (row) in the input data is independently normalized across its features.\n",
    "\n",
    "Normalization Along Rows:\n",
    "With axis=0, normalization is applied along each feature (column) across all the samples (rows).\n",
    "Each feature's mean and standard deviation are calculated across all the samples, considering all the values of that feature across all samples.\n",
    "The normalization is then applied to each sample's feature values based on these aggregated statistics.\n",
    "\n",
    "\"\"\"\n",
    "x_train= tf.keras.utils.normalize(x_train, axis= 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95026b70-4fbd-4c0f-8cf0-79aa134f9559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Test data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#check the shape of what x_train and x_test is\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f280cd8-3f46-4917-a93c-e0644779c5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(10, 10))\\nfor i in range(25):  # Plotting the first 25 images\\n    plt.subplot(5, 5, i + 1)\\n    plt.imshow(x_train[i], cmap='gray')\\n    plt.axis('off')  # Turn off axis labels\\n    plt.title(f'Label: {y_train[i]}')  # Show the label as the title\\nplt.show()\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot some of the images from the training set\n",
    "\"\"\"\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):  # Plotting the first 25 images\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.title(f'Label: {y_train[i]}')  # Show the label as the title\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb639007-f19b-4dd2-b526-73ef71c0217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28))) #flattens the data turns the pixels from 28x28 to a line between 784\n",
    "#flatten turns it from 2-D to a 1-D array\n",
    "\n",
    "#add relu layer and softmax/optimization functions below:\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) #model.add(tf.keras.layers.Dense(128,activation = 'relu')) relu follows something simlar to\n",
    "#sigmoid activation relu = f(x)=max(0,x) It returns 0 for all negative inputs and returns the input value for all positive inputs.\n",
    "\n",
    "#relu is the activation function in the dense layer and 128 is the number of units\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) # make a second layer \n",
    "model.add(tf.keras.layers.Dense(10,activation = tf.nn.softmax)) # now the softmax layer all the 10 neurons will add up to one \n",
    "#softmax is the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "749b4845-6c52-4e8b-9a99-1bfa4e83a315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#how relu works:\\nimport numpy as np\\n\\n# Define a sample input array\\ninput_array = np.array([-2, -1, 0, 1, 2])\\n\\n# Apply ReLU activation function\\noutput_array = np.maximum(0, input_array)\\n\\nprint(\"Input array:\", input_array)\\nprint(\"Output array (ReLU):\", output_array)\\n\\n#Input array: [-2 -1  0  1  2]\\n#Output array (ReLU): [0 0 0 1 2]\\n\\n\\n#how softmax works:\\nimport numpy as np\\n\\ndef softmax(logits):\\n    exp_logits = np.exp(logits)\\n    softmax_scores = exp_logits / np.sum(exp_logits)\\n    return softmax_scores\\n\\n# Example logits (raw scores)\\nlogits = np.array([2.0, 1.0, 0.1])\\n\\n# Compute softmax probabilities\\nprobabilities = softmax(logits)\\n\\nprint(\"Raw Scores (Logits):\", logits)\\nprint(\"Softmax Probabilities:\", probabilities)\\nprint(\"Sum of Probabilities:\", np.sum(probabilities))\\n\\n#Raw Scores (Logits): [2.  1.  0.1]\\n#Softmax Probabilities: [0.65900114 0.24243297 0.09856589]\\n#Sum of Probabilities: 1.0\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#how relu works:\n",
    "import numpy as np\n",
    "\n",
    "# Define a sample input array\n",
    "input_array = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Apply ReLU activation function\n",
    "output_array = np.maximum(0, input_array)\n",
    "\n",
    "print(\"Input array:\", input_array)\n",
    "print(\"Output array (ReLU):\", output_array)\n",
    "\n",
    "#Input array: [-2 -1  0  1  2]\n",
    "#Output array (ReLU): [0 0 0 1 2]\n",
    "\n",
    "\n",
    "#how softmax works:\n",
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_scores = exp_logits / np.sum(exp_logits)\n",
    "    return softmax_scores\n",
    "\n",
    "# Example logits (raw scores)\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Compute softmax probabilities\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Raw Scores (Logits):\", logits)\n",
    "print(\"Softmax Probabilities:\", probabilities)\n",
    "print(\"Sum of Probabilities:\", np.sum(probabilities))\n",
    "\n",
    "#Raw Scores (Logits): [2.  1.  0.1]\n",
    "#Softmax Probabilities: [0.65900114 0.24243297 0.09856589]\n",
    "#Sum of Probabilities: 1.0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc321a51-7222-430c-9979-1da548cb93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#uses adam optimization make sure you spell the loss function right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af34520e-d1c4-462c-ae48-257ab2b1fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\mikec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mikec\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 958us/step - loss: 0.0583 - accuracy: 0.9816\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 2s 955us/step - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 2s 960us/step - loss: 0.0318 - accuracy: 0.9895\n",
      "INFO:tensorflow:Assets written to: handwritten.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: handwritten.model\\assets\n"
     ]
    }
   ],
   "source": [
    "#fit the model pass the training data\n",
    "\n",
    "model.fit(x_train,y_train, epochs=3) \n",
    "# epochs is how many iterations the model will see the data\n",
    "\n",
    "#you can use clusting nearest neighbor instead / convolutional neural netoworks or\n",
    "\n",
    "model.save('handwritten.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd824ab3-ce9f-4790-9d5f-daf76aa3a191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 847us/step - loss: 0.1020 - accuracy: 0.9730\n",
      "Test Loss: 0.10195577144622803\n",
      "Test Accuracy: 0.9729999899864197\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('handwritten.model')\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab7983-3e03-4a70-a0f3-db5243c92547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
