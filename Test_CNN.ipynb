{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f623ed9-93b9-4b0f-9ade-8efef1cb4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor flow guide book https://archive.org/details/ai-ml/page/24/mode/2up\n",
    "import tensorflow as tf # for ML and we will be using a tensorflow data set/ the mnist number data set\n",
    "#tensorflow guide https://www.tensorflow.org/guide/keras/sequential_model\n",
    "import numpy as np #working with arrays\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import cv2 #for computer vision\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e711bc5-4345-418b-9313-15f06d5e170b",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a67d161c-3310-4795-ba01-371b378c69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using my own data this time https://www.youtube.com/watch?v=q7ZuZ8ZOErE\n",
    "#we will still greyscale data reason being it takes the intensity of the light to formulate image recognition processes.\n",
    "#the data will be stored in folders and subfolders\n",
    "#------------\n",
    "#libraries\n",
    "import os \n",
    "os.environ[\"Tf_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c2099-3288-4e11-9b5d-8de3f6d4635a",
   "metadata": {},
   "source": [
    "The TF_CPP_MIN_LOG_LEVEL environment variable takes integer values from 0 to 3:\n",
    "\n",
    "0: Default, shows all logs (including INFO, WARNING, ERROR, and FATAL).\n",
    "1: Filter out INFO logs; only WARNING, ERROR, and FATAL are shown.\n",
    "2: Filter out INFO and WARNING logs; only ERROR and FATAL are shown.\n",
    "3: Filter out INFO, WARNING, and ERROR logs; only FATAL is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25965259-9b3c-4a5d-a9ab-300e05c56a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify test sizes\n",
    "\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "batch_size= 1 # small because small data set will need to adjust as we get larger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dccbf3fc-4824-4365-8aa4-fd672ca853ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image files in directory:\n",
      "C:\\Users\\mikec\\openmath\\Training\\1\\1_0.png\n",
      "C:\\Users\\mikec\\openmath\\Training\\1\\1_1.png\n",
      "C:\\Users\\mikec\\openmath\\Training\\2\\2_0.png\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "07865780-5d84-4b78-a534-6c99ba686914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 files belonging to 2 classes.\n",
      "Using 3 files for training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found in directory C:\\Users\\mikec\\openmath\\Training. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Creating the dataset from a directory \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmikec\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mopenmath\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#catagorical binary\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#class_names = [1,2],\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m123\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# if you want validation set and training set then you need to set the seed\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m ds_validation \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     subset\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#ignore the error\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\image_dataset.py:303\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    300\u001b[0m     image_paths, labels, validation_split, subset\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_paths:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo images found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAllowed formats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALLOWLIST_FORMATS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m dataset \u001b[38;5;241m=\u001b[39m paths_and_labels_to_dataset(\n\u001b[0;32m    309\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mimage_paths,\n\u001b[0;32m    310\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mimage_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m     crop_to_aspect_ratio\u001b[38;5;241m=\u001b[39mcrop_to_aspect_ratio,\n\u001b[0;32m    317\u001b[0m )\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: No images found in directory C:\\Users\\mikec\\openmath\\Training. Allowed formats: ('.bmp', '.gif', '.jpeg', '.jpg', '.png')"
     ]
    }
   ],
   "source": [
    "#Creating the dataset from a directory \n",
    "\n",
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'C:\\\\Users\\\\mikec\\\\openmath\\\\Training',\n",
    "    labels = 'inferred',\n",
    "    label_mode= \"int\", #catagorical binary\n",
    "    #class_names = [1,2],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    image_size=(image_height, image_width),\n",
    "    shuffle = True,\n",
    "    seed = 123, # if you want validation set and training set then you need to set the seed\n",
    "    validation_split= 0.1,\n",
    "    subset= \"training\",\n",
    ")\n",
    "\n",
    "\n",
    "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Training\",\n",
    "    labels = 'inferred',\n",
    "    label_mode= \"int\", #catagorical binary\n",
    "    #class_names = [1,2],\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = batch_size,\n",
    "    image_size=(image_height, image_width),\n",
    "    shuffle = True,\n",
    "    seed = 123, # if you want validation set and training set then you need to set the seed\n",
    "    validation_split= 0.1,\n",
    "    subset= \"validation\",\n",
    ")\n",
    "#ignore the error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742441a-6362-46cd-bafc-72bfe381584e",
   "metadata": {},
   "source": [
    "The seed parameter is used to initialize the random number generator used by\n",
    "TensorFlow's dataset processing functions. By setting a seed, you ensure that the random operations, \n",
    "such as shuffling the data or splitting the dataset into training and validation sets, are performed in a reproducible way.\n",
    "This means that every time you run your script with the same seed, the random operations will produce the same results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7f79875-477e-41df-9e89-6dcc323a53fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mrandom_brightness(x,max_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image,y\n\u001b[1;32m----> 7\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mds_train\u001b[49m\u001b[38;5;241m.\u001b[39mmap(augment)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "#x is our features y is our classes\n",
    "#augment reduces lighting dependency\n",
    "def augment(x,y):\n",
    "    image = tf.image.random_brightness(x,max_delta=0.05)\n",
    "    return image,y\n",
    "\n",
    "ds_train = ds_train.map(augment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50cba86-cc6b-4693-8f39-e37d7157db04",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f1d42b-86cf-4b14-b76a-ca3db9637217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28))) #flattens the data turns the pixels from 28x28 to a line between 784\n",
    "#flatten turns it from 2-D to a 1-D array\n",
    "\n",
    "#add relu layer and softmax/optimization functions below:\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) #model.add(tf.keras.layers.Dense(128,activation = 'relu')) relu follows something simlar to\n",
    "#sigmoid activation relu = f(x)=max(0,x) It returns 0 for all negative inputs and returns the input value for all positive inputs.\n",
    "\n",
    "#relu is the activation function in the dense layer and 128 is the number of units\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) # make a second layer \n",
    "model.add(tf.keras.layers.Dense(10,activation = tf.nn.softmax)) # now the softmax layer all the 10 neurons will add up to one \n",
    "#softmax is the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429df58-5e0f-4b40-8092-d087a13e395c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f096c2c5-9781-4a0b-857b-21fb404aa1de",
   "metadata": {},
   "source": [
    "image: This is a tensor representing the input image.\n",
    "max_delta: This parameter controls the range of the random brightness adjustment. \n",
    "It specifies the maximum absolute difference between the original brightness value and the adjusted brightness value. \n",
    "For example, max_delta=0.2 means that the brightness will be adjusted by a random value between -0.2 and 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63079f-d8c1-4917-a614-b98b496f0ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579077d-ba1f-44ee-94cb-d5220ae806ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#uses adam optimization make sure you spell the loss function right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010443fe-3cd5-40fd-a8ec-99fa5088c265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f542e2b-840d-4938-93f1-0a20b8d5648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61df227-5d51-4360-84c8-15c4b0b7a75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1651c-0e89-4fc9-9a4f-b6103f9e4ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a5e64-1165-454c-ae84-23cdbcdd5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_________________\"\"\"stop here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50f083-c675-4a6d-98eb-7cc6eb5d1b5b",
   "metadata": {},
   "source": [
    "\n",
    "When we train our own data\n",
    "using sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " Assuming you have your data in variables x (images) and y (labels)\n",
    "\n",
    " Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    " test_size=0.2 indicates that 20% of the data will be used for testing\n",
    " random_state is used to ensure reproducibility, setting it to a fixed value ensures the same random split each time you run the code\n",
    "\n",
    "for the datapipeline it would probably benefit from using tf.datasets framework for images \n",
    "pandas for statistical data \n",
    "refer to this page:\n",
    "https://www.tensorflow.org/guide/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440a7b5-21c2-46c0-b011-5d3b7646cf41",
   "metadata": {},
   "source": [
    "#now lets normalize the data/grey scaling \n",
    "\"\"\"gray scale pixel has value of  255  normalizing would make it between 0 and 1 \n",
    "it will be scaling things down the value of 125 might look like 0.5\"\"\"\n",
    "\n",
    "\"\"\"axis=-1:\n",
    "When axis=-1, normalization is performed along the last axis of the input data.\n",
    "In the context of a 2D tensor (like a matrix), where the first axis corresponds to rows and the second axis corresponds to columns, axis=-1 implies normalization is performed along the columns.\n",
    "This means that each feature (column) in the input data is independently normalized across the samples.\n",
    "axis=1:\n",
    "When axis=1, normalization is performed along the second axis of the input data.\n",
    "In the context of a 2D tensor, where the first axis corresponds to rows and the second axis corresponds to columns, axis=1 implies normalization is performed along the rows.\n",
    "This means that each sample (row) in the input data is independently normalized across its features.\n",
    "\n",
    "Normalization Along Rows:\n",
    "With axis=0, normalization is applied along each feature (column) across all the samples (rows).\n",
    "Each feature's mean and standard deviation are calculated across all the samples, considering all the values of that feature across all samples.\n",
    "The normalization is then applied to each sample's feature values based on these aggregated statistics.\n",
    "\n",
    "\"\"\"\n",
    "x_train= tf.keras.utils.normalize(x_train, axis= 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25518a-c6da-4223-88b2-3435c03e2b49",
   "metadata": {},
   "source": [
    "#check the shape of what x_train and x_test is\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f280cd8-3f46-4917-a93c-e0644779c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some of the images from the training set\n",
    "\"\"\"\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):  # Plotting the first 25 images\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.title(f'Label: {y_train[i]}')  # Show the label as the title\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a1009-6cb1-49ab-94ed-6ee210fffd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this is convulitonal one to do larger scaled\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (None, None, 1)  # Allow variable input size for height and width, and 1 channel for grayscale\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "model.add(tf.keras.layers.Rescaling(1./255))  # Rescale pixel values to [0, 1]\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "# Global average pooling\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb639007-f19b-4dd2-b526-73ef71c0217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (28,28))) #flattens the data turns the pixels from 28x28 to a line between 784\n",
    "#flatten turns it from 2-D to a 1-D array\n",
    "\n",
    "#add relu layer and softmax/optimization functions below:\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) #model.add(tf.keras.layers.Dense(128,activation = 'relu')) relu follows something simlar to\n",
    "#sigmoid activation relu = f(x)=max(0,x) It returns 0 for all negative inputs and returns the input value for all positive inputs.\n",
    "\n",
    "#relu is the activation function in the dense layer and 128 is the number of units\n",
    "model.add(tf.keras.layers.Dense(128,activation = tf.nn.relu)) # make a second layer \n",
    "model.add(tf.keras.layers.Dense(10,activation = tf.nn.softmax)) # now the softmax layer all the 10 neurons will add up to one \n",
    "#softmax is the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9314035-15a7-4d37-afb1-d8a9544afa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#how relu works:\n",
    "import numpy as np\n",
    "\n",
    "# Define a sample input array\n",
    "input_array = np.array([-2, -1, 0, 1, 2])\n",
    "\n",
    "# Apply ReLU activation function\n",
    "output_array = np.maximum(0, input_array)\n",
    "\n",
    "print(\"Input array:\", input_array)\n",
    "print(\"Output array (ReLU):\", output_array)\n",
    "\n",
    "#Input array: [-2 -1  0  1  2]\n",
    "#Output array (ReLU): [0 0 0 1 2]\n",
    "\n",
    "\n",
    "#how softmax works:\n",
    "import numpy as np\n",
    "\n",
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits)\n",
    "    softmax_scores = exp_logits / np.sum(exp_logits)\n",
    "    return softmax_scores\n",
    "\n",
    "# Example logits (raw scores)\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "# Compute softmax probabilities\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Raw Scores (Logits):\", logits)\n",
    "print(\"Softmax Probabilities:\", probabilities)\n",
    "print(\"Sum of Probabilities:\", np.sum(probabilities))\n",
    "\n",
    "#Raw Scores (Logits): [2.  1.  0.1]\n",
    "#Softmax Probabilities: [0.65900114 0.24243297 0.09856589]\n",
    "#Sum of Probabilities: 1.0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc321a51-7222-430c-9979-1da548cb93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#uses adam optimization make sure you spell the loss function right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190fca79-30f2-4e95-b6a3-3f3c0a8154b0",
   "metadata": {},
   "source": [
    "Cross-entropy is a concept from information theory and is commonly used as a loss function in machine learning, particularly in classification tasks. Mathematically, cross-entropy measures the difference between two probability distributions: the predicted probability distribution and the true probability distribution.\n",
    "\n",
    "Given:\n",
    "\n",
    "p(y): The true probability distribution of the classes. For example, in binary classification,  p(y) could be a one-hot encoded vector representing the true class label.\n",
    "\n",
    "q(y): The predicted probability distribution of the classes. For example, output of a softmax function representing predicted class probabilities.\n",
    "\n",
    "The cross-entropy between \n",
    "p and q, denoted as H(p,q), is defined as:\n",
    "H(p, q) = -\\sum_{i} p(y_i) \\log(q(y_i))\n",
    "Where:\n",
    "\n",
    "i iterates over all classes,\n",
    "\n",
    "p(y_i)  is the true probability of class i\n",
    "\n",
    "q(y_i) is the predicted probability of class i\n",
    "\n",
    "In binary classification, where  p(y) is a one-hot encoded vector with the true class label, the cross-entropy simplifies to the following form:\n",
    "H(p, q) = -[p(y) \\log(q(y)) + (1 - p(y)) \\log(1 - q(y))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34520e-d1c4-462c-ae48-257ab2b1fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model pass the training data\n",
    "\n",
    "model.fit(x_train,y_train, epochs=3) \n",
    "# epochs is how many iterations the model will see the data\n",
    "\n",
    "#you can use clusting nearest neighbor instead / convolutional neural netoworks or\n",
    "\n",
    "model.save('handwritten.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd824ab3-ce9f-4790-9d5f-daf76aa3a191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('handwritten.model')\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "#low loss and high accuracy is better \n",
    "#will write to a .pb file with the name of handwritten model \n",
    "#pb stands for proto buffer\n",
    "# protobuffer is data type like json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56599aa-42cf-4bea-a2fc-99438f58646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#use os stream and paint to test this model\n",
    "\n",
    "\n",
    "#https://www.youtube.com/watch?v=bte8Er0QhDg need fo figure out why the path isnt working\n",
    "image_number = 0\n",
    "predictions = []\n",
    "\n",
    "while os.path.isfile(f\"Numbers/number{image_number}.png\"):\n",
    "    try:\n",
    "        img = cv2.imread(f\"Numbers/number{image_number}.png\")[:,:,0]\n",
    "        #img = cv2.imread(f\"foldername/label{image_number}.png\")[::0]#we dont care about colors so [:,:,0] will be white on black inverted\n",
    "        #need to invert it to be black on white use numpy to invert the numbers\n",
    "        img = np.invert(np.array([img])) \n",
    "        #img will be an array of a list of img\n",
    "        prediction = model.predict(img)\n",
    "        print(f\"the number is probably {np.argmax(prediction)}\") #argmax gives index of the field of with the highest number which neuron has the highest activation\n",
    "        #zeroth with represent zero the first will represent 1\n",
    "        predictions.append(np.argmax(prediction))\n",
    "        plt.imshow(img[0],cmap=plt.cm.binary)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"error\")\n",
    "    finally: #nomatter what this will execute\n",
    "        image_number+=1\n",
    "\n",
    "#if model doesn't work well train with more epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76736d3e-44b0-4f7d-b1d3-81968b6f7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import cv2\n",
    "\n",
    "# Specify the path to the image file\n",
    "file_path = \"C:\\\\Users\\\\mikec\\\\openmath\\\\Numbers\\\\number1.png\"\n",
    "\n",
    "# Read the image using OpenCV\n",
    "img = cv2.imread(file_path)\n",
    "\n",
    "# Check if the image was successfully loaded\n",
    "if img is not None:\n",
    "    # Display the image in a window\n",
    "    cv2.imshow('Number 1', img)\n",
    "    # Wait for any key press to close the window\n",
    "    cv2.waitKey(0)\n",
    "    # Close all OpenCV windows\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Failed to load the image.\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfce85-2f35-4e23-85fc-d74251c480cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf272f-4e7e-4f5c-b733-5095642c3200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
